本文档主要用于梳理集成学习相关知识点。

## 1.Bagging ##

Bagging基本流程，有放回地采样出T个含有m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合,对预测输出进行结合时，通常采用简单投票法（分类任务），通常采用简单平均法（回归任务）；

结合策略：
1. 平均法：1）简单平均法；2）加权平均法；
2. 投票法：1）绝对多数投票法；2）相对多数投票法；3）加权投票法；
3. 学习法

### 1.1 Random Forest ###

Random Forest是Bagging的一个扩展体，RF在以决策树为基学习器构建Bagging的基础上，进一步在决策树的训练过程中引入了随机属性选择；

两个随机：1）样本随机采样；2）属性随机选择；

## 2.Boosting ##

Boosting是一族可将弱学习器提升为强学习器的算法。这族算法的工作机制：先从初始训练集中训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复进行，直至基学习器数目达到事先指定的值T，最终将这T个基学习器进行**加权**结合。Boosting族算法最著名的代表是AdaBoost。

Boosting算法要求基学习器能对特定的数据分布进行学习，这可通过“重赋权法”(re-weighting)实施，即在训练过程的每一轮中，根据样本分布为每个训练样本重新赋予一个权重。对无法接受带权样本的基学习算法，则可通过“重采样法”(re-sampling)来处理，即在每一轮学习中，根据样本分布对训练集重新进行采样，再用重采样而得的样本集对基学习器进行训练。一般而言，这两种做法没有显著的优劣差别。

### 2.1 AdaBoost ###

AdaBoost算法有多种推导方式，比较容易理解的是基于“加性模型”(additive model)，即基学习器的线性组合

$H(x)=\sum_{t=1}^{T}\alpha_{t} * h_{t}(x)$

来最小化指数损失函数(exponential loss function)

$\ell_{exp}(H|D)=\mathbb{E}_{x \backsim D}[e^{-f(x)H(x)}]$

AdaBoost算法基本流程，

****
**输入：** 训练集D={(x1,y1),(x2,y2),...,(xm,ym)};

基学习算法c；

训练轮数T；

**过程:**

    1:初始化样本权值分布；
    2:for t = 1,2,3..T do
    3:    基于分布Dt从数据集D中训练出分类器ht;
    4:    估计ht的误差；
    5:    if 误差大于0.5 then break；
    6:    确定分类器ht的权重；
    7:    更新样本分布；
    8:end for

**输出:** $H(x) = sign(\sum_{t=1}^{T}\alpha_{t}h_{t}(x))$；

1:$D_{1}(x)=1/m$；

3:$h_{t}(x)=c(D,D_{t})$；

4:$\epsilon_{t}=P_{x \backsim D_{t}}(h_{t}(x) \neq f(x))$；

6:$\alpha_{t}=\frac{1}{2}ln(\frac{1-\epsilon_{t}}{\epsilon_{t}})$；

7:$D_{t+1}(x)=\frac{D_{t}(x)}{Z_{t}}*exp(-\alpha_{t}),h_{t}(x) = f(x)$；

$D_{t+1}(x)=\frac{D_{t}(x)}{Z_{t}}*exp(\alpha_{t}),h_{t}(x) \neq f(x)$；

$D_{t+1}(x)=\frac{D_{t}(x)*exp(-\alpha_{t}f(x)h_{t}(x))}{Z_{t}}$；

$Z_{t}$ 是规范化因子，以确保Dt+1是一个分布；

****

### 2.2 提升树 ###

提升树是以分类树或者回归树为基本分类器的提升方法。

提升方法实际采用加性模型(即基学习器的线性组合)与前向分步算法。以决策树为基学习器的提升算法称为提升树(boosting tree)。对分类问题决策树是二叉分类数，对回归问题决策树是二叉回归树。

针对不同问题的提升树算法，主要区别在于使用不同的损失函数，包括平方误差损失函数的回归问题，指数损失函数的分类问题，以及用一般损失函数的一般决策问题。

回归问题的提升树算法：

***
**输入：** 训练集D={(x1,y1),(x2,y2),...,(xN,yN)};

训练轮数M；

**过程:**

    1:初始化；
    2:for m = 1,2,3..M do
    3:    计算各个样本的残差;
    4:    拟合残差，学习一个回归树；
    5:    更新；
    6:end for

**输出:** $f_{M}(x) = \sum_{m=1}^{M}T(x;\Theta_{m})$；

1:$f_{0}(x)=0$；

3:$r_{mi}=y_{i}-f_{m-1}(x_{i}),i=1,2,3,...N$；

4:$T(x;\Theta_{m})$ 即为学习的回归树；

5:$f_{m}(x)=f_{m-1}(x)+T(x;\Theta_{m})$用于更新；

***

### 2.3 GBDT ###
